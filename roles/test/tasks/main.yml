---
# NB: this only works on centos 8 / ohpc v2 as we want UCX
# TODO: add support for groups/partitions

# Cleanup limitations: app packages not removed from login, nfs packages aren't removed from all hosts (might have been installed for something else), exports isn't reverted

- name: Create test root directory
  file:
    path: "{{ openhpc_tests_rootdir }}"
    state: directory
    owner: "{{ ansible_user }}"
    mode: 0755
  become: yes
  run_once: yes
  tags: always, setup

- name: Install nfs packages
  yum:
    name: nfs-utils
    state: present
  become: yes
  tags: always, setup # TODO: add this to more

- name: Temporarily export application directores from login
  command:
    cmd: "exportfs -o rw,insecure,no_root_squash *:{{ item }}"
  loop: "{{ openhpc_tests_app_dirs }}"
  become: yes
  when: ansible_hostname == openhpc_slurm_login
  # NB is exporting directories which don't exist ok?

- name: Ensure nfs services are running
  service:
    name: nfs-server
    state: started
  become: yes
  when: ansible_hostname == openhpc_slurm_login

- name: Ensure application directories exist
  file:
    path: "{{ item }}"
    state: directory
    # assumes the parent exists, that's ok here
  loop: "{{ openhpc_tests_app_dirs }}"
  become: yes
  when: ansible_hostname != openhpc_slurm_login

- name: Temporarily mount application directories
  command:
    cmd: "mount -t nfs {{ openhpc_slurm_login }}:{{ item }} {{ item }}"
  loop: "{{ openhpc_tests_app_dirs }}"
  become: yes
  when: ansible_hostname != openhpc_slurm_login

- name: Get info about compute nodes
  shell: "sinfo --Node --noheader{%if openhpc_tests_nodes is defined %} --nodes {{openhpc_tests_nodes}}{% endif %} --format %N"
  register:
    computes
  changed_when: false
  tags: always
  failed_when: computes.rc != 0 or (computes.stdout_lines | length == 0)
  run_once: yes

- name: IMB PingPong (2x scheduler-selected nodes)
  block:
    - include: pingpong.yml
  tags: pingpong
  vars:
    jobdir: "{{ openhpc_tests_rootdir }}/pingpong"
  when: inventory_hostname == openhpc_slurm_login

- name: Ping matrix (all selected nodes)
  block:
    - include: pingmatrix.yml
  tags: pingmatrix
  vars:
    jobdir: "{{ openhpc_tests_rootdir }}/pingmatrix"
  when: inventory_hostname == openhpc_slurm_login

- name: HPL (individual nodes)
  # NB: This uses the precompiled HPL provided with MKL so runs a single MPI process per node, with TBB threads added automatically by MKL to use all cores
  # See https://software.intel.com/content/www/us/en/develop/documentation/mkl-windows-developer-guide/top/intel-math-kernel-library-benchmarks/intel-distribution-for-linpack-benchmark/ease-of-use-command-line-parameters.html
  block:
    - include: hpl-solo.yml
  tags: hpl-solo
  vars:
    jobdir: "{{ openhpc_tests_rootdir }}/hpl-solo"
    impi_ver: 2019.6-088 # NB: not exposing these as role vars as not all work!
    mkl_ver: 2020.0-088
  when: inventory_hostname == openhpc_slurm_login

- name: HPL (individual nodes)
  # NB: This uses the precompiled HPL provided with MKL so runs a single MPI process per node, with TBB threads added automatically by MKL to use all cores
  # See https://software.intel.com/content/www/us/en/develop/documentation/mkl-windows-developer-guide/top/intel-math-kernel-library-benchmarks/intel-distribution-for-linpack-benchmark/ease-of-use-command-line-parameters.html
  block:
    - include: hpl-all.yml
  tags: hpl-all
  vars:
    jobdir: "{{ openhpc_tests_rootdir }}/hpl-all"
    impi_ver: 2019.6-088 # NB: not exposing these as role vars as not all work!
    mkl_ver: 2020.0-088
  when: inventory_hostname == openhpc_slurm_login

- name: Unmount application directories
  command:
    cmd: "umount {{ item }}"
  loop: "{{ openhpc_tests_app_dirs }}"
  become: yes
  when: ansible_hostname != openhpc_slurm_login

- name: Unexport application directories
  command:
    cmd: "exportfs -u *:{{ item }}"
  loop: "{{ openhpc_tests_app_dirs }}"
  become: yes
  when: ansible_hostname == openhpc_slurm_login
